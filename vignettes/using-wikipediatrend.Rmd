---
title: "Using Wikipediatrend"
author: "Peter Meißner"
date: "`r Sys.Date()`"
output:
  rmarkdown::html_vignette:
    fig_caption: yes
    fig_width: 7 
    fig_height: 5
    number_sections: yes
    toc: yes
    toc_depth: 2
vignette: >
  %\VignetteIndexEntry{Using Wikipediatrend}
  %\VignetteEngine{knitr::rmarkdown}
  \usepackage[utf8]{inputenc}
---

```{r general options, include=FALSE}
  # default line color in ggplot graphics
    library(ggplot2)
    update_geom_defaults("line",   list(colour = "steelblue"))
  
  # use cache coming with package to make vignette build by magnitudes faster 
  # there still we be some downloads of data so that building the vignette should imply 
  # that reproducing the vignette by user is possible
    library(wikipediatrend)
    wp_cache_load(
      system.file(
        "extdata", 
        "wikipediatrend_cache.csv", 
        package = "wikipediatrend"
      )
    )
```



> What do Wikipedia's readers care about? Is Britney Spears more popular than Brittany? Is Asia Carrera more popular than Asia? How many people looked at the article on Santa Claus in December? How many looked at the article on Ron Paul?
> <br> What can you find?
> <br> *Source:* http://stats.grok.se/ 



The *wikipediatrend* package provides convenience
access to daily page view counts (*Wikipedia article traffic statistics*)
stored at http://stats.grok.se/ . 

If you want to know how often an article has been viewed over time and 
work with the data from within R, this package is for you. 
Maybe you want to compare how much attention articles from different languages got and when, 
this package is for you.
Are you up to policy studies or epidemiology? Have a look at page counts for *Flue*, *Ebola*, *Climate Change* or
*Millennium Development Goals* and maybe build a model or two. Again, this package is for you. 

If you simply want to browse Wikipedia page view statistics without all that coding,
visit http://stats.grok.se/ and have a look around. 

If non-big data is not an option, get the raw data in their entity at 
http://dumps.wikimedia.org/other/pagecounts-raw/ . 

If you think days are crude measures of time but seconds might do if need be and info about which article views led to the numbers is useless anyways - go to http://datahub.io/dataset/english-wikipedia-pageviews-by-second. 

To get further information on the data source (Who? When? How? How good?) there is a Wikipedia article for that: http://en.wikipedia.org/wiki/Wikipedia:Pageview_statistics and another one: http://en.wikipedia.org/wiki/Wikipedia:About_page_view_statistics .



# Installation

**stable CRAN version**

```{r, message=F, eval=FALSE}
install.packages("wikipediatrend")
```

**developemnt version**

```{r, message=F, eval=FALSE}
devtools::install_github("petermeissner/wikipediatrend")
```

**... and load it via:**


```{r, message=F}
library(wikipediatrend)
```


# A first try

The workhorse of the package is the `wp_trend()` function that allows you to get page 
view counts as neat data frames like this:

```{r, message=F}
page_views <- wp_trend("main_page")
    
page_views
```

... that can easily be turned into a plot ... 


```{r, message=F}
library(ggplot2)

ggplot(page_views, aes(x=date, y=count)) + 
  geom_line(size=1.5, colour="steelblue") + 
  geom_smooth(method="loess", colour="#00000000", fill="#001090", alpha=0.1) +
  scale_y_continuous( breaks=c(10e6, 15e6, 20e6), 
  label=c("10 M","15 M","20 M")) +
  theme_bw()
```


# `wp_trend()` options

`wp_trend()` has several options and most of them are set to defaults: 

page , 
from        = prev_month_start(), 
to          = prev_month_end(),
lang        = "en", 
file        = wp_cache_file(), 


- `page`
- `from = Sys.Date() - 30`
- `to   = Sys.Date()`
- `lang = "en"`
- `file = wp_cache_file()`
- ~~`friendly`~~ *deprecated*
- ~~`requestFrom`~~ *deprecated*
- ~~`userAgent`~~ *deprecated*

## `page`

The `page` option allows to specify one or more article titles for which data should be retrieved. 

These titles should be in the same format as shown in the address bar of your browser to ensure that the pages are found. 
If we want to get page views for the United Nations Millennium Development Goals and 
the article is found here:  *"http://en.wikipedia.org/wiki/Millennium_Development_Goals"* the page title to pass to `wp_trend()` should be *Millennium_Development_Goals* not *Millennium Development Goals* or *Millennium_development_goals* or amy other *'mostly-like-the-original'* variation. 

To ease data gathering `wp_trend()` `page` accepts whole vectors of page  titles and will retrieve date for each one after another. 

```{r, message=F}
page_views <- 
  wp_trend( 
    page = c( "Millennium_Development_Goals", "Climate_Change") 
  )
```

```{r, message=F}
library(ggplot2)

ggplot(page_views, aes(x=date, y=count, group=page, color=page)) + 
  geom_line(size=1.5) + theme_bw()
```


## `from` and `to`

These two options determine the time frame for which data shall be retrieved. The defaults are set to gather the last 30 days but might be set to cover larger time frames as well. Note that there is no data prior to December 2007 so that any date prior will be set to this minimum. 

```{r, message=F}
page_views <- 
  wp_trend( 
    page = "Millennium_Development_Goals" ,
    from = "2000-01-01",
    to   = prev_month_end()
  )
```

```{r, message=F, warning=FALSE}
library(ggplot2)

ggplot(page_views, aes(x=date, y=count, color=wp_year(date))) + 
  geom_line() + 
  stat_smooth(method = "lm", formula = y ~ poly(x, 22), color="#CD0000a0", size=1.2) +
  theme_bw() 
```


## `lang`

This option determines for which Wikipedia the page views shall be retrieved, English, German, Chinese, Spanish, ... . The default is set to `"en"` for the English Wikipedia. This option should get one language shorthand that then is used for all pages or for each page a corresponding language shorthand should be specified. 

```{r, message=F}
page_views <- 
  wp_trend( 
    page = c("Objetivos_de_Desarrollo_del_Milenio", "Millennium_Development_Goals") ,
    lang = c("es", "en"),
    from = Sys.Date()-100
  )
```

```{r, message=F}
library(ggplot2)

ggplot(page_views, aes(x=date, y=count, group=lang, color=lang, fill=lang)) + 
  geom_smooth(size=1.5) + 
  geom_point() +
  theme_bw() 
```


## `file`

This last option allows for storing the data retrieved by a call to `wp_trend()`
in a file, e.g. `file = "MyCache.csv"`. While `MyCache.csv` will be created if it 
does not exist already it will never be overwritten by `wp_trend()` thus allowing 
to accumulate data from susequent calls to `wp_trend()`. To get the data stored 
back into R use `wp_load(file = "MyCache.csv")`.

```{r, message=FALSE}
wp_trend("Cheese", file="cheeeeese.csv")
wp_trend("Käse", lang="de", file="cheeeeese.csv")

cheeeeeese <- wp_load( file="cheeeeese.csv" )
cheeeeeese
```

```{r, include=FALSE}
file.remove("cheeeeese.csv")
```



# Caching

## Session caching

When using `wp_trend()` you will notice that subsequent calls to the function 
might take considerably less time than previous calls - given that later 
calls include data that has been downloaded already. This is due to the caching
system running in the background and keeping track of things downloaded already. 
You can see if `wp_trend()` had to download something if it reports one or more 
links to the stats.grok.se server, e.g. ... 

```{r}
wp_trend("Cheese")
wp_trend("Cheese")
```
... but ... 

```{r}
wp_trend("Cheese", from = Sys.Date()-60)
```

The current cache in memory can be accessed via:

```{r}
wp_get_cache()
```

... and emptied by a call to `wp_cache_reset()`.


## Caching across sessions 1

While everything that is downloaded during a session is cached in memory it might come handy to save the cache parallel on disk to reuse it in the next R session.  To activate disk-caching for a session simply use:


```{r, eval=FALSE}
wp_set_cache_file( file = "myCache.csv" )
```

The function will reload whatever is stored in the file and in subsequent calls to 
`wp_trend()` will automatically add data as it is downloaded. The file used for disk-caching can be changed by another call to `wp_set_cache_file( file = "myOtherCache.csv")` or turned off completely by leaving the `file` argument empty. 


## Caching across sessions 2

If disk-caching should be enabled by default one can define a path as system/environment variable `WP_CACHE_FILE`. When loading the package it will look for this variable via `Sys.getenv("WP_CACHE_FILE")` and use the path for caching 
as if ...

```{r, eval=FALSE}
wp_set_cache_file( Sys.getenv("WP_CACHE_FILE") )
```

.. would have beend typed in by the user. 



# Counts for other languages

If comparing languages is important one needs to specify the exact article titles for each language: While the article about the Millennium Goals has an English title in the English Wikipedia, it of course is named differently in Spanish, German, Chinese, ... . One might look these titles up by hand or use the handy `wp_linked_pages()` function like this:


```{r, message=F}
titles <- wp_linked_pages("Islamic_State_of_Iraq_and_the_Levant", "en")
titles <- titles[titles$lang %in% c("en", "de", "es", "ar", "ru"),]
titles 
```

... then we can use the information to get data for several languages ... 

```{r, message=F}
page_views <- 
  wp_trend(
    page = titles$page[1:5], 
    lang = titles$lang[1:5],
    from = "2014-08-01"
  )
```


```{r, message=F}
library(ggplot2)

for(i in unique(page_views$lang) ){
  iffer <- page_views$lang==i
  page_views[iffer, ]$count <- scale(page_views[iffer, ]$count)
}

ggplot(page_views, aes(x=date, y=count, group=lang, color=lang)) + 
  geom_line(size=1.2, alpha=0.5) + 
  ylab("standardized count\n(by lang: m=0, var=1)") +
  theme_bw() + 
  scale_colour_brewer(palette="Set1") + 
  guides(colour = guide_legend(override.aes = list(alpha = 1)))
```


# Going beyond Wikipediatrend -- Anomalies and mean shifts 



## Identifying anomalies with `AnomalyDetection` 

Currently the `AnomalyDetection` package is not availible on CRAN so we have to use `install_github()` from the `devtools` package to get it. 

```{r}
# install.packages( "AnomalyDetection", repos="http://ghrr.github.io/drat",  type="source")
library(AnomalyDetection)
library(dplyr)
library(ggplot2)
```

The package is a little picky about the data it accepts for processing so we 
have to build a new data frame. It should contain only the date and count variable.
Furthermore, `date` should be named `timestamp` and transformed to type `POSIXct`.

```{r}
page_views <- wp_trend("Syria", from = "2010-01-01")

page_views_br <- 
  page_views  %>% 
  select(date, count)  %>% 
  rename(timestamp=date)  %>% 
  unclass()  %>% 
  as.data.frame() %>% 
  mutate(timestamp = as.POSIXct(timestamp))
```

Having transformed the data we can detect anomalies via `AnomalyDetectionTs()`. 
The function offers various options e.g. the significance level for rejecting 
normal values (`alpha`); the maximum fraction of the data that is allowed to be 
detected as anomalies (`max_amoms`); whether or not upward deviations, downward 
devaitions or irregularities in both directions might form the basis of anomaly 
detection (`direction`) and last but not least whether or not the time frame for 
detection is larger than one month (`lonterm`).

Lets choose a greedy set of parameters and detect possible anomalies:

```{r}
res <- 
AnomalyDetectionTs(
  x         = page_views_br, 
  alpha     = 0.05, 
  max_anoms = 0.40,
  direction = "both",
  longterm  = T
)$anoms

res$timestamp <- as.Date(res$timestamp)

head(res)
```

... and play back the detected anomalies to our `page_views` data set:

```{r} 
page_views <- 
  page_views  %>% 
  mutate(normal = !(page_views$date %in% res$timestamp))  %>% 
  mutate(anom   =   page_views$date %in% res$timestamp )

class(page_views) <- c("wp_df", "data.frame")
```

Now we can plot counts and anomalies ... 

```{r, message=FALSE}
(
  p <-
    ggplot( data=page_views, aes(x=date, y=count) ) + 
      geom_line(color="steelblue") +
      geom_point(data=filter(page_views, anom==T), color="red2", size=2) +
      theme_bw()
)
```

... as well as compare running means: 

```{r, message=FALSE}
p + 
  geom_line(stat = "smooth", size=2, color="red2", alpha=0.7) + 
  geom_line(data=filter(page_views, anom==F), 
  stat = "smooth", size=2, color="dodgerblue4", alpha=0.5) 
```

It seems like upward and downward anomalies partial each other out most of the 
time since both smooth lines (with and without anomalies) do not differ much. 
Nonetheless, keeping anomalies in will upward bias the counts slightly, so we 
proceed with a cleaned up data set: 

```{r}
page_views_clean <- 
  page_views  %>% 
  filter(anom==F)  %>% 
  select(date, count, lang, page, rank, month, title)

page_views_br_clean <- 
  page_views_br  %>% 
  filter(page_views$anom==F)
```



## Identifying mean shifts with `BreakoutDetection` 

`BreakoutDetection` is a package that allows to search data for mean level shifts
by dividing it into timespans of change and those of stability in the presence 
of seasonal noise. 
Similar to `AnomalyDetection` the `BreakoutDetection` package is not available 
on CRAN but has to be obtained from Github. 


```{r}
# install.packages(  "BreakoutDetection",   repos="http://ghrr.github.io/drat", type="source")
library(BreakoutDetection)
library(dplyr)
library(ggplot2)
library(magrittr)
```

... again the workhorse function (`breakout()`) is picky and requires *"a data.frame which has 'timestamp' and 'count' components"* like our `page_views_br_clean`. 

The function has two general options: one tweaks the minimum length of a timespan 
(`min.size`); the other one does determine how many mean level changes might 
occur during the whole time frame (`method`); and several method specific 
options, e.g. `decree`, `beta`, and `percent` which control the sensitivity 
adding further breakpoints. In the following case the last option tells 
the function that overall model fit should be increased by at least 5 percent 
if adding a breakpoint. 

```{r}
br <- 
  breakout(
    page_views_br_clean, 
    min.size = 30, 
    method   = 'multi', 
    percent  = 0.05,
    plot     = TRUE
  )
br
```

In the following snippet we combine the break information with our page views data and can have a look at the dates at which the breaks occured. 

```{r}
breaks <- page_views_clean[br$loc,]
breaks

```

Next, we add a span variable capturing which page_view observations belong to 
which span, allowing us to aggregate data. 

```{r}
page_views_clean$span <- 0
for (d in breaks$date ) {
  page_views_clean$span[ page_views_clean$date > d ] %<>% add(1)
}

page_views_clean$mcount <- 0
for (s in unique(page_views_clean$span) ) {
  iffer <- page_views_clean$span == s
page_views_clean$mcount[ iffer ] <- mean(page_views_clean$count[iffer])
}

spans <- 
  page_views_clean  %>% 
  as_data_frame() %>% 
  group_by(span) %>% 
  summarize(
    start      = min(date), 
    end        = max(date), 
    length     = end-start,
    mean_count = round(mean(count)),
    min_count  = min(count),
    max_count  = max(count),
    var_count  = var(count)
  )
spans
```

Also, we can now plot the shifting mean. 

```{r, message=FALSE}
ggplot(page_views_clean, aes(x=date, y=count) ) + 
  geom_line(alpha=0.5, color="steelblue") + 
  geom_line(aes(y=mcount), alpha=0.5, color="red2", size=1.2) + 
  theme_bw()
```




















































